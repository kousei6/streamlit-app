import streamlit as st
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import re
import nltk  # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’import

st.set_page_config(
    page_title="Text Summarizer",
    page_icon="âœï¸",
    layout="wide"
)

# NLTKã®ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸€åº¦ã ã‘å®Ÿè¡Œï¼‰
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    st.warning("Downloading stopwords corpus...")
    nltk.download('stopwords')
    st.success("Stopwords corpus downloaded successfully.")

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    st.warning("Downloading punkt tokenizer...")
    nltk.download('punkt')
    st.success("Punkt tokenizer downloaded successfully.")

# ğŸ”¹ punkt_tab ã‚‚è¿½åŠ ï¼ˆæ–°ã—ã„NLTKä»•æ§˜ã§å¿…è¦ï¼‰
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    st.warning("Downloading punkt_tab tokenizer...")
    nltk.download('punkt_tab')
    st.success("Punkt_tab tokenizer downloaded successfully.")

def summarize_text(text, n=3):
    """
    TF-IDFã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¾ã™ã€‚
    
    Args:
        text (str): è¦ç´„ã™ã‚‹å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã€‚
        n (int): ç”Ÿæˆã™ã‚‹è¦ç´„æ–‡ã®æ•°ã€‚
        
    Returns:
        str: è¦ç´„ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‚
    """
    if not text.strip():
        return ""

    # å‰å‡¦ç†ï¼šå¥èª­ç‚¹ã€æ•°å­—ã€å°æ–‡å­—åŒ–
    cleaned_text = re.sub(r'[^a-zA-Z\s.]', '', text.lower())
    
    # æ–‡ç« ã«åˆ†å‰²
    sentences = sent_tokenize(cleaned_text)
    if len(sentences) <= n:
        # å…ƒã®æ–‡ç« æ•°ãŒæŒ‡å®šã•ã‚ŒãŸè¦ç´„æ–‡æ•°ã‚ˆã‚Šå°‘ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        return text

    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®å–å¾—ï¼ˆè‹±èªï¼‰
    stop_words = set(stopwords.words('english'))
    
    # TF-IDFãƒ™ã‚¯ã‚¿ãƒ©ã‚¤ã‚¶ã®åˆæœŸåŒ–
    vectorizer = TfidfVectorizer(stop_words=list(stop_words))
    
    # TF-IDFè¡Œåˆ—ã‚’è¨ˆç®—
    tfidf_matrix = vectorizer.fit_transform(sentences)
    
    # å„æ–‡ç« ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆå„æ–‡ç« ã®TF-IDFå€¤ã®åˆè¨ˆï¼‰
    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()
    
    # ã‚¹ã‚³ã‚¢ã®é«˜ã„æ–‡ç« ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    top_sentence_indices = sentence_scores.argsort()[-n:][::-1]
    
    # å…ƒã®æ–‡ç« ã®é †ç•ªã§ã‚½ãƒ¼ãƒˆ
    sorted_indices = sorted(top_sentence_indices)
    
    # è¦ç´„æ–‡ã‚’ç”Ÿæˆ
    summary_sentences = [sentences[i] for i in sorted_indices]
    
    return " ".join(summary_sentences)

st.title("âœï¸ Text Summarizer")
st.write("é•·ã„æ–‡ç« ã‚’è‡ªå‹•çš„ã«è¦ç´„ã—ã¾ã™ã€‚")

st.info("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€**TF-IDF** (Term Frequency-Inverse Document Frequency) ã¨ã„ã†æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€æ–‡ç« ã®é‡è¦åº¦ã‚’è¨ˆç®—ã—ã€æœ€ã‚‚é‡è¦ãªæ–‡ç« ã‚’æŠ½å‡ºã—ã¦è¦ç´„ã‚’ä½œæˆã—ã¾ã™ã€‚
""")

# å¿…è¦ãªå¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è¡¨ç¤º
st.info("ã“ã®ã‚¢ãƒ—ãƒªã«ã¯`nltk`ã¨`scikit-learn`ãŒå¿…è¦ã§ã™ã€‚`pip install nltk scikit-learn`ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
input_text = st.text_area(
    "ã“ã“ã«è¦ç´„ã—ãŸã„æ–‡ç« ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚",
    height=400,
    placeholder="""
(ã“ã“ã«é•·ã„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ä¾‹ï¼š)
Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. Leading AI textbooks define the field as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.
As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be "AI," having become a routine technology.
"""
)

# è¦ç´„æ–‡ã®æ•°ã‚’æŒ‡å®š
summary_length = st.slider(
    "è¦ç´„ã™ã‚‹æ–‡ç« ã®æ•°",
    min_value=1,
    max_value=10,
    value=3
)

if st.button("è¦ç´„ã‚’å®Ÿè¡Œ"):
    if input_text:
        with st.spinner("è¦ç´„ä¸­..."):
            summary = summarize_text(input_text, summary_length)
            st.markdown("---")
            st.subheader("è¦ç´„çµæœ")
            if summary:
                st.success(summary)
            else:
                st.warning("è¦ç´„ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å…¥åŠ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        st.warning("æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
